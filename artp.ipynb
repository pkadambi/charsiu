{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d854fc-7456-465f-817a-4d0ee064dc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/prad/anaconda3/envs/alignment/lib/python3.7/site-packages/transformers/configuration_utils.py:359: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    }
   ],
   "source": [
    "from src.Charsiu import charsiu_forced_aligner\n",
    "from alignment_helper_fns import *\n",
    "from scipy.io import wavfile\n",
    "import torch\n",
    "import soundfile as sf\n",
    "CHARSIU_MODEL = charsiu_forced_aligner('charsiu/en_w2v2_fc_10ms')\n",
    "\n",
    "#datafiles\n",
    "TRANSCRIPT = 'The chairman decided to pave over the shopping center garden'\n",
    "audio_filepath = '/home/prad/github/charsiu/data/artp_files/chairman_healthy.wav'\n",
    "# audio_filepath = '/home/prad/github/charsiu/data/artp_files/chariman_healthy.wav'\n",
    "tg_filepath = '/home/prad/github/charsiu/data/artp_files/chairman_healthy.TextGrids'\n",
    "\n",
    "#load charsiu mdoel\n",
    "CHARSIU_MODEL.serve(audio = audio_filepath, text = TRANSCRIPT, save_to = tg_filepath)\n",
    "PHONE2IND = CHARSIU_MODEL.charsiu_processor.processor.tokenizer.encoder\n",
    "# \n",
    "# aligned_phone_df = textgridpath_to_phonedf(txtgrid_path=tg_filepath, phone_key='phones', replace_silence=True, remove_numbers=True)\n",
    "# audio_signal, fs = sf.read(audio_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c9359c8-cc1e-4d9f-96bf-319fd99e85dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nArtp function \\n\\nHere are the steps to computing Artp\\n\\n\\n1. Generate the sequence of phones from the text \\n2. Generate the framewise phone labels\\n3. Generate the probability distribution for each frame\\n4. For each frame, calculate the expected/maximum probability\\n\\n--\\n5. Group by each phoneme\\n\\n\\n\\n\\n---\\nMy improvements\\n6. a windowed version of artp\\n\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Artp function \n",
    "\n",
    "Here are the steps to computing Artp\n",
    "\n",
    "\n",
    "1. Generate the sequence of phones from the text \n",
    "2. Generate the framewise phone labels\n",
    "3. Generate the probability distribution for each frame\n",
    "4. For each frame, calculate the expected/maximum probability\n",
    "\n",
    "--\n",
    "5. Group by each phoneme\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "My improvements\n",
    "6. a windowed version of artp\n",
    "\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b04f73-464f-4594-a2b9-79cde1fbad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' get logprobas from healthy aligner'''\n",
    "\n",
    "def get_logprobas(audio_path, charsiu):\n",
    "    audio_signal, fs = sf.read(audio_path)\n",
    "    assert fs == 16000\n",
    "    # charsiu.aligner._get_feat_extract_output_lengths(len(audio_signal))\n",
    "    audio = charsiu.charsiu_processor.audio_preprocess(audio_signal)\n",
    "    inputs = torch.tensor(audio).float().unsqueeze(0).to(charsiu.device)\n",
    "    with torch.no_grad():\n",
    "        out = charsiu.aligner(inputs)\n",
    "    logits = out[0]\n",
    "    logprobas = torch.log_softmax(logits, dim=-1).detach().cpu().numpy().squeeze()\n",
    "    return logprobas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0142b11-1d38-4951-8825-5384d047edcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[SIL]': 0, 'NG': 1, 'F': 2, 'M': 3, 'AE': 4, 'R': 5, 'UW': 6, 'N': 7, 'IY': 8, 'AW': 9, 'V': 10, 'UH': 11, 'OW': 12, 'AA': 13, 'ER': 14, 'HH': 15, 'Z': 16, 'K': 17, 'CH': 18, 'W': 19, 'EY': 20, 'ZH': 21, 'T': 22, 'EH': 23, 'Y': 24, 'AH': 25, 'B': 26, 'P': 27, 'TH': 28, 'DH': 29, 'AO': 30, 'G': 31, 'L': 32, 'JH': 33, 'OY': 34, 'SH': 35, 'D': 36, 'AY': 37, 'S': 38, 'IH': 39, '[UNK]': 40, '[PAD]': 41}\n"
     ]
    }
   ],
   "source": [
    "print(CHARSIU_MODEL.charsiu_processor.processor.tokenizer.encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b7d4d9-1c84-49a2-b86f-460423ab346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' get aligned phone ids from aligned df'''\n",
    "phone_df = textgridpath_to_phonedf(txtgrid_path=tg_filepath, phone_key='phones', replace_silence=True, remove_numbers=True)\n",
    "# transcript_phone_ids = charsiu.charsiu_processor.get_phones_and_words(TRANSCRIPT)\n",
    "\n",
    "\n",
    "def return_aligned_phns_and_ids(aligned_df, charsiu, seqlen, phone2id_dict=PHONE2IND):\n",
    "    timesteps = .01 * np.arange(seqlen) + .01\n",
    "    # timesteps = .01 * np.arange(530)\n",
    "\n",
    "    aligned_phns = []\n",
    "\n",
    "\n",
    "\n",
    "    for timestep in timesteps:\n",
    "        gt_start_indicator = timestep > aligned_df.iloc[:, 0].values\n",
    "        lt_end_indicator = timestep <= aligned_df.iloc[:, 1].values\n",
    "        match_indicator = np.logical_and(gt_start_indicator, lt_end_indicator)\n",
    "        aligned_ind = np.argwhere(match_indicator).ravel()\n",
    "        # print(aligned_ind)\n",
    "        # framewise_phns.append()\n",
    "        if len(aligned_ind)==0:\n",
    "            _phn = '[SIL]'\n",
    "        elif len(aligned_ind)>1:\n",
    "            Exception('Error found more than 1 matching phone index!')\n",
    "        else:\n",
    "            _phn = aligned_df.iloc[aligned_ind, 2].values[0]\n",
    "            _phn = '[SIL]' if _phn=='sil' else _phn\n",
    "            # print(_phn)\n",
    "            aligned_phns.append(_phn)\n",
    "    aligned_phn_idxs = [phone2id_dict[phone] for phone in aligned_phns]\n",
    "    return np.array(aligned_phns), np.array(aligned_phn_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aad09ca-975c-4840-b6cb-0a49efa5e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_GOP(logprobas, aligned_phn_idxs, sil_phone='[SIL]', phone_subset=None, ignore_sil=True, phone2id_dict=PHONE2IND):\n",
    "    \n",
    "    max_phn_idxs = np.argmax(logprobas, axis=1)\n",
    "    framewise_gop = np.array([logprobas[ii, align_phn_idx] - logprobas[ii, max_phn_idx] for ii, (align_phn_idx, max_phn_idx) in enumerate(zip(aligned_phn_idxs, max_phn_idxs))])\n",
    "    sil_phn_idx = phone2id_dict[sil_phone]    \n",
    "    nosil_idxs = np.argwhere(aligned_phn_idxs!=sil_phn_idx).ravel()\n",
    "    framewise_gop_nosil = framewise_gop[nosil_idxs]\n",
    "    output = {'framewise_gop': framewise_gop_nosil, 'gopmean': np.mean(framewise_gop_nosil), 'gop_withsil': np.mean(framewise_gop)}\n",
    "\n",
    "    if phone_subset is not None:\n",
    "        framewise_subset_gops = np.array([])\n",
    "        for phone in phone_subset:\n",
    "            _phn_idx = phone2id_dict[phone]\n",
    "            phone_idxs = np.argwhere(aligned_phn_idxs==_phn_idx).ravel()\n",
    "            framewise_subset_gops = np.concatenate([framewise_subset_gops, framewise_gop[phone_idxs]]) #don't need to use framewise_gops_nosil since we're filtering phones anyways\n",
    "            output['framewise_gop_subset'] = framewise_subset_gops\n",
    "            output['gopmean_subset'] = np.mean(framewise_subset_gops)\n",
    "    return output\n",
    "    #TODO: implement for phone_subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed9ee113-8bd8-4d84-8625-e838af0cd346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AA', 'AH', 'AY', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'G', 'IH',\n",
       "       'M', 'N', 'NG', 'OW', 'P', 'R', 'S', 'SH', 'T', 'UW', 'V', '[SIL]'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio = charsiu.charsiu_processor.audio_preprocess(audio_signal)\n",
    "# inputs = torch.tensor(audio).float().unsqueeze(0).to(charsiu.device)\n",
    "audio_signal\n",
    "phones, words = CHARSIU_MODEL.align(audio_signal, text=TRANSCRIPT)\n",
    "phndf = pd.DataFrame.from_records(phones, columns = ['start', 'end', 'phone'])\n",
    "np.unique(phndf['phone'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "106b278b-d0ae-4c71-94b5-8c27b5b9e0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO implement for phone subset\n",
    "\n",
    "def get_aligner_frame_seq_len(audio_filepath, fs, charsiu):\n",
    "    audio = charsiu.charsiu_processor.audio_preprocess(audio_filepath, sr=fs)\n",
    "    audio = torch.Tensor(audio).unsqueeze(0).to(charsiu.device)\n",
    "    print(len(audio))\n",
    "    return charsiu.aligner._get_feat_extract_output_lengths(audio)\n",
    "    \n",
    "def calculate_GOP_e2e(audio_filepath, transcript, charsiu_model, phone_subset=None):\n",
    "    audio_signal, fs = sf.read(audio_filepath)\n",
    "    phones, words, logits = charsiu_model.align(audio_signal, text=TRANSCRIPT, return_logits=True)\n",
    "    aligned_phone_df = pd.DataFrame.from_records(phones, columns = ['start', 'end', 'phone'])\n",
    "    seqlen = CHARSIU_MODEL.aligner._get_feat_extract_output_lengths(len(audio_signal))\n",
    "    print(seqlen)\n",
    "    aligned_phns, aligned_phn_idxs = return_aligned_phns_and_ids(aligned_phone_df, seqlen=seqlen, charsiu=charsiu_model)\n",
    "    logprobas = torch.log_softmax(logits, dim=-1).numpy()\n",
    "    gopoutput = _calc_GOP(logprobas, aligned_phn_idxs, phone_subset=phone_subset)\n",
    "    if phone_subset is not None:\n",
    "        return gopoutput['gopmean_subset']\n",
    "    else:\n",
    "        return gopoutput['gopmean']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db8c25de-cf57-4e24-acd6-e351da531824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prad/github/charsiu/data/artp_files/chairman_healthy.wav'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b6adda97-fb00-4682-a43e-f1fa52cf6316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(530)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.26588184"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_GOP_e2e(audio_filepath=audio_filepath, transcript=TRANSCRIPT, charsiu_model=CHARSIU_MODEL, phone_subset=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab0b80d9-bb7d-4afd-aaf0-77506b7eece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "framewise_gop = [logprobas[ii, align_phn_idx] - logprobas[ii, max_phn_idx] for ii, (align_phn_idx, max_phn_idx) in enumerate(zip(aligned_phn_ids, max_phn_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "543214c7-e5db-4650-927a-d53acf1de500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17508069"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(framewise_gop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a246ba80-a811-40aa-bb02-12f27067d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_ids = charsiu.charsiu_processor.get_phone_ids(aligned_phns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "727d4ea5-b5a0-43da-9f86-bfb6a009ea83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sil'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aligned_phns[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1c07c87d-27a2-4e2a-9f52-ef138e68836a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.3203125"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Healthy file'''\n",
    "\n",
    "\n",
    "''' Healthy file older male'''\n",
    "\n",
    "\n",
    "''' ALS Speaker'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df311a-0297-4442-a26b-0894134537e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Run initial GOP compute on Gabi's files'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d171d423-f8ac-4baf-a184-6ec61658590e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.iloc[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ca6fb-bc2c-4c1c-b4d5-b07c21202544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "alignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
