{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "047b4f0d-4c44-46a6-8a82-054e5878f937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from alignment_helper_fns import textgridpath_to_phonedf, get_all_textgrids_in_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cbfbf2b-71a6-4252-80c6-2437fda53105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all textgrids in directory:\t ./results/phonation_baseline_frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1233.98it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./for_yan.csv')\n",
    "phonation_dataset_path = '/home/prad/datasets/phonation_data'\n",
    "\n",
    "estimated_tgs = get_all_textgrids_in_directory('./results/phonation_baseline_frame')\n",
    "estimated_phone_dfs = [textgridpath_to_phonedf(tgpath, phone_key='phones', replace_silence=False) for tgpath in estimated_tgs]\n",
    "audio_paths = [os.path.join(phonation_dataset_path, tgname.split('/')[-1][:-8]+'wav') for tgname in estimated_tgs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9689c0a-5ace-4ec6-9f8c-5fb377622989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 59)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d28473-529f-4437-9cf8-0896b6f9ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading audio files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 168/168 [00:00<00:00, 1294.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting transcripts, phone and word bounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 168/168 [00:00<00:00, 3104154.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Framewise Labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 168/168 [00:02<00:00, 60.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phonation_dataset.PhonationDataset at 0x7f20f6846550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from phonation_dataset import *\n",
    "tg_dir = phonation_dataset_path+'/extracted_manual_textgrids'\n",
    "PhonationDataset(audio_paths=audio_paths, lables_df=df, textgrids_dir=tg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1135b9b-3063-4b39-ac06-7e1e60591c4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sessionStepId', 'session.sessionId', 'steps.stepId', 'steps.filePath',\n",
       "       'steps.stepCategory', 'steps.prompt', 'start_time_groundtruth',\n",
       "       'end_time_groundtruth', 'session.sessionScriptId', 'steps.name',\n",
       "       'steps.sequenceNumber', 'steps.elicitType', 'session.language',\n",
       "       'sessionId', 'stepId', 'RatePhonationStepMaxVoicedDuration',\n",
       "       'RatePhonationStepMaxVoicedDurationNoAmbient',\n",
       "       'RatePhonationStepPauseCount', 'RatePhonationStepPauseCountNoAmbient',\n",
       "       'RatePhonationStepPauseMean', 'RatePhonationStepPauseMeanNoAmbient',\n",
       "       'RatePhonationStepPausePercent',\n",
       "       'RatePhonationStepPausePercentNoAmbient',\n",
       "       'RatePhonationStepSpeechEndTime',\n",
       "       'RatePhonationStepSpeechEndTimeNoAmbient',\n",
       "       'RatePhonationStepSpeechStartTime',\n",
       "       'RatePhonationStepSpeechStartTimeNoAmbient',\n",
       "       'RatePhonationStepSpeechTotalDuration',\n",
       "       'RatePhonationStepSpeechTotalDurationNoAmbient',\n",
       "       'RatePhonationStepTotalVoicedDuration',\n",
       "       'RatePhonationStepTotalVoicedDurationNoAmbient',\n",
       "       'RateSentenceStepArticulationSpeed',\n",
       "       'RateSentenceStepArticulationSpeedNoAmbient',\n",
       "       'RateSentenceStepMaxVoicedDuration',\n",
       "       'RateSentenceStepMaxVoicedDurationNoAmbient',\n",
       "       'RateSentenceStepPauseCount', 'RateSentenceStepPauseCountNoAmbient',\n",
       "       'RateSentenceStepPauseMean', 'RateSentenceStepPauseMeanNoAmbient',\n",
       "       'RateSentenceStepPausePercent', 'RateSentenceStepPausePercentNoAmbient',\n",
       "       'RateSentenceStepSpeechEndTime',\n",
       "       'RateSentenceStepSpeechEndTimeNoAmbient', 'RateSentenceStepSpeechSpeed',\n",
       "       'RateSentenceStepSpeechSpeedNoAmbient',\n",
       "       'RateSentenceStepSpeechStartTime',\n",
       "       'RateSentenceStepSpeechStartTimeNoAmbient',\n",
       "       'RateSentenceStepTotalSpeechDuration',\n",
       "       'RateSentenceStepTotalSpeechDurationNoAmbient',\n",
       "       'RateSentenceStepTotalVoicedDuration',\n",
       "       'RateSentenceStepTotalVoicedDurationNoAmbient', 'asr_engine_aws',\n",
       "       'asr_start_time_aws', 'asr_end_time_aws', 'transcript_aws',\n",
       "       'asr_engine_google', 'asr_start_time_google', 'asr_end_time_google',\n",
       "       'transcript_google'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})\n",
    "# list(tdf[idx].to_records(index=False))\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ac1c9da-49c3-4a15-b408-853f1399c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phonation_dataset import *\n",
    "# PhonationDataset(audio_paths=audio_paths, lables_df=df, textgrids_dir='/home/prad/datasets/phonation_data/extracted_manual_textgrids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a263e-92fe-4a97-893b-a7ea88a9d113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1572da4b-e2c6-439e-8197-d4fd5e2fb323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb327b3b-c49a-499e-8e6d-d9af41535f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cdc1191-55cd-406e-9eb4-04a7e62a1646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.26191, 21.54974)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_startend_from_audiopath(audiopath, datadf):\n",
    "    sessid = audiopath.split('/')[-1][:-4]\n",
    "    return get_startend_time_from_sessid(sessid, datadf)\n",
    "def get_startend_time_from_sessid(sessid, datadf, sessid_key='sessionStepId'):\n",
    "    # sess_index = \n",
    "    row = datadf.iloc[np.argwhere(sessid==datadf[sessid_key].values).ravel()]\n",
    "    return float(row['start_time_groundtruth']), float(row['end_time_groundtruth'])\n",
    "    \n",
    "# print(df.iloc[np.argwhere(audio_paths[idx].split('/')[-1][:-4]==df['sessionStepId'].values).ravel()])\n",
    "get_startend_from_audiopath(audio_paths[idx], datadf=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77cdff32-f381-4c38-b7f2-950cbd2a313e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(np.argwhere(df['RatePhonationStepPauseCount'].values!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05a8c990-e063-495b-b372-10000b77ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n",
      "38\n",
      "(240, 59)\n",
      "Extracting all textgrids in directory:\t ./results/phonation_baseline_frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 593.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "Num Pauses: 0.0\n",
      "./results/phonation_baseline_frame/1042-20171216-0947-7.TextGrid\n",
      "(0.51795, 12.29527)\n",
      "/home/prad/datasets/phonation_data/1042-20171216-0947-7.wav\n",
      "       0      1      2\n",
      "0   0.00   0.52  [SIL]\n",
      "1   0.52  12.28     AA\n",
      "2  12.28  15.40  [SIL]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "phonation_dataset_path = '/home/prad/datasets/phonation_data'\n",
    "df['steps.filePath'] = [os.path.join(phonation_dataset_path, filename.split('/')[-1]) for filename in df['steps.filePath']]\n",
    "pause_colname = 'RatePhonationStepPauseCount'\n",
    "print(sum(df['RatePhonationStepPauseCount']==0.))\n",
    "print(sum(df['RatePhonationStepPauseCount']==1.))\n",
    "print(df.shape)\n",
    "end_time_key = 'RatePhonationStepSpeechEndTimeNoAmbient'\n",
    "start_time_key = 'RatePhonationStepSpeechStartTime'\n",
    "\n",
    "\n",
    "idx = 30\n",
    "estimated_tgs = get_all_textgrids_in_directory('./results/phonation_baseline_frame')\n",
    "idx = int(np.argwhere(['1042-20171216-0947-7' in tg for tg in estimated_tgs]))\n",
    "print(idx)\n",
    "estimated_phone_dfs = [textgridpath_to_phonedf(tgpath, phone_key='phones', replace_silence=False) for tgpath in estimated_tgs]\n",
    "audio_paths = [os.path.join(phonation_dataset_path, tgname.split('/')[-1][:-8]+'wav') for tgname in estimated_tgs]\n",
    "print('Num Pauses:', df['RatePhonationStepPauseCount'][idx])\n",
    "print(estimated_tgs[idx])\n",
    "print(get_startend_from_audiopath(audio_paths[idx], datadf=df))\n",
    "print(audio_paths[idx])\n",
    "print(estimated_phone_dfs[idx])\n",
    "# print(df[np.argwhere(audio_paths[idx].split('/')[-1][:-3] in df['sessionStepId'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e8b4f32-9375-4c4f-a118-f054cdde62e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paused_estimated_tgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5013a4-d55b-42f4-93b7-66f14cdde5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "any([paused_sessids[0] in _tg for _tg in estimated_tgs])\n",
    "print(paused_sessids[0])\n",
    "print(len(estimated_tgs))\n",
    "print(estimated_tgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c4fbd0b-cbd5-42b6-ab06-3eee164c673c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting all textgrids in directory:\t ./results/phonation_baseline_frame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 760.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Num Pauses: 1.0\n",
      "./results/phonation_baseline_frame/5022-031089-03-20181126-0049-10.TextGrid\n",
      "(1.54327, 19.10958)\n",
      "/home/prad/datasets/phonation_data/5022-001008-13-20180821-0747-10.wav\n",
      "       0      1      2\n",
      "0   0.00   1.54  [SIL]\n",
      "1   1.54   8.48     AA\n",
      "2   8.48   8.85  [SIL]\n",
      "3   8.85  19.05     AA\n",
      "4  19.05  20.51  [SIL]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phonation_dataset_path = '/home/prad/datasets/phonation_data'\n",
    "df['steps.filePath'] = [os.path.join(phonation_dataset_path, filename.split('/')[-1]) for filename in df['steps.filePath']]\n",
    "pause_colname = 'RatePhonationStepPauseCount'\n",
    "end_time_key = 'RatePhonationStepSpeechEndTimeNoAmbient'\n",
    "start_time_key = 'RatePhonationStepSpeechStartTime'\n",
    "\n",
    "# idx = 30\n",
    "paused_df = df[df[pause_colname]!=0]\n",
    "paused_df = paused_df.reset_index()\n",
    "# paused_df = paused_df['sessionStepId']\n",
    "paused_sessids = list(paused_df['sessionStepId'].values)\n",
    "estimated_tgs = get_all_textgrids_in_directory('./results/phonation_baseline_frame')\n",
    "paused_estimated_tgs = [tg for tg in estimated_tgs if any([psid in tg for psid in paused_sessids])]\n",
    "# idx = int(np.argwhere(['1042- in tg for tg in estimated_tgs]))\n",
    "idx = 0\n",
    "print(idx)\n",
    "estimated_phone_dfs = [textgridpath_to_phonedf(tgpath, phone_key='phones', replace_silence=False) for tgpath in paused_estimated_tgs]\n",
    "audio_paths = [os.path.join(phonation_dataset_path, tgname.split('/')[-1][:-8]+'wav') for tgname in paused_estimated_tgs]\n",
    "print('Num Pauses:', paused_df['RatePhonationStepPauseCount'][idx])\n",
    "print(estimated_tgs[idx])\n",
    "print(get_startend_from_audiopath(audio_paths[idx], datadf=paused_df))\n",
    "print(audio_paths[idx])\n",
    "print(estimated_phone_dfs[idx])\n",
    "# print(df[np.argwhere(audio_paths[idx].split('/')[-1][:-3] in df['sessionStepId'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc0af3b-e1d9-470d-a9cd-3b516b5724e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 29.814)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65481560-aa08-4b3b-b73f-1f798b3c43fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alignment",
   "language": "python",
   "name": "alignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
